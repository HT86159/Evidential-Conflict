# Visual-Hallucination-Detection-in-Large-Vision-Language-Models-via-Evidential-Conflict
This repository contains the code and resources necessary to reproduce our experiments on detecting hallucinations in multi-modal large language models using evidential conflict and semantic entropy. Our research focuses on evaluating model uncertainty and its relation to hallucination occurrences in LLaVA and mPLUG-Owl series models.

## System Requipment
We here discuss hardware and software system requirements.

### Hardware Dependencies
GPU acceleration is crucial for reproducing our experiments within a reasonable timeframe, and the GPU model used is the NVIDIA RTX 3090 with 24GB of VRAM; note that experiments without a GPU may take significantly longer and are not recommended.

### Software Dependencies
The experiments require Python 3.10, PyTorch 2.0.1, and Ubuntu 20.04.4 LTS.

## Installation Guide
We conducted experiments with LLaVA models including the 7B and 13B variants of LLaVA-v1.5, as well as the 34B LLaVA-v1.6 (since LLaVA-v1.5 has no 34B version), all performed on NVIDIA RTX 3090 GPUs. For enhanced inference speed, we recommend implementing 4-bit quantization, which notably reduces inference time while preserving acceptable accuracy.
For the deployment of each model and the setup of the corresponding virtual environment, please follow the installation instructions published by the official Github repository of each model.

## Demo
At present, we have not yet developed a Demo.

## Further Instructions

### Repository Structure
This respository is devided into five files, which are "infer", "infer_results", "measures", "models", "model_weights".
Among them, the "infer" folder stores the code for model inference. 
The "infer_results" folder stores the results generated by running the model inference files.
The "model_weights" folder stores the weights of the last hidden layer of each model.
The "models" folder stores the files required for the deployment of each model.
The "measures" folder contains code for processing model inference results, including code for verifying the correctness of model outputs using GPT-4o, code for calculating various uncertainty metrics, and code for computing evaluation metrics such as AUROC and ACC on the final results.
