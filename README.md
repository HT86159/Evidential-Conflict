# Visual-Hallucination-Detection-in-Large-Vision-Language-Models-via-Evidential-Conflict
This repository contains the code and resources necessary to reproduce our experiments on detecting hallucinations in multi-modal large language models using evidential conflict and semantic entropy. Our research focuses on evaluating model uncertainty and its relation to hallucination occurrences in LLaVA and mPLUG-Owl series models.

## System Requipment
We here discuss hardware and software system requirements.

### Hardware Dependencies
Generally speaking, our experiments require modern computer hardware which is suited for usage with Large Vision-Language Models (LVLMs).
Replicating our experiments within a reasonable timeframe would be challenging without a GPU.
We used NVIDIA RTX 3090 GPUs for all tests, evaluating LLaVA models of 7B, 13B, 34B scales and mPLUG-Owl2, mPLUG-Owl3.
Specifically, the 7B/13B models were LLaVA-v1.5, while the 34B model adopted LLaVA-v1.6 (since v1.5 lacks a 34B variant), all executed on RTX 3090s.
For accelerated inference, 4-bit quantization is recommended to significantly reduce processing time while maintaining acceptable accuracy.

### Software Dependencies
Our experiments utilized Python 3.10, PyTorch 2.0.1, and the Ubuntu 20.04.4 LTS operating system.

## Installation Guide
For the deployment of mPLUG-Owl2, mPLUG-Owl3, and LLaVA models, please refer to the official documentation of each model for self-installation and deployment. Since our experiments involve extracting information from the Transformers decoder, reproducing our work requires modifying the Transformers source code to simultaneously return the model's output probability distribution and the state of the decoder's final hidden layer during inference. When using Evidential Conflict to quantify the uncertainty of large language models, model weight files are required. For ease of computation, we have placed these weight files on Hugging Face at: https://huggingface.co/datasets/thuang5288/PRE-HAL/tree/main/model_weights.



## Demo
At present, we have not yet developed a Demo.

## Further Instructions
Due to the inconsistent output formats of large language models (where responses may not include option letters like A, B, C, D), we strongly recommend using the GPT-4o API for auxiliary judgment. The standard experimental workflow is as follows: first, use the large model for inference and store the required experimental data; second, call the GPT-4o API to match the model's responses with provided options, determining correctness (0 for no hallucination, 1 for hallucination); third, use measure.py to calculate uncertainty metrics and save the results; finally, compute the AUROC (Area Under the Receiver Operating Characteristic Curve) score based on the correctness labels and uncertainty metrics.

### Repository Structure
This respository is devided into five files, which are "infer", "infer_results", "measures", "models", "model_weights".
Among them, the "infer" folder stores the code for model inference. 
The "infer_results" folder stores the results generated by running the model inference files.
The "model_weights" folder stores the weights of the last hidden layer of each model.
The "models" folder stores the files required for the deployment of each model.
The "measures" folder contains code for processing model inference results, including code for verifying the correctness of model outputs using GPT-4o, code for calculating various uncertainty metrics, and code for computing evaluation metrics such as AUROC and ACC on the final results.
