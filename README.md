# Visual-Hallucination-Detection-in-Large-Vision-Language-Models-via-Evidential-Conflict
This repository contains the code and resources necessary to reproduce our experiments on detecting hallucinations in multi-modal large language models using evidential conflict and semantic entropy. Our research focuses on evaluating model uncertainty and its relation to hallucination occurrences in LLaVA and mPLUG-Owl series models.

## System Requipment
We here discuss hardware and software system requirements.

### Hardware Dependencies
Generally speaking, our experiments require modern computer hardware which is suited for usage with Large Vision-Language Models (LVLMs).
Replicating our experiments within a reasonable timeframe would be challenging without a GPU.
We used NVIDIA RTX 3090 GPUs for all tests, evaluating LLaVA models of 7B, 13B, 34B scales and mPLUG-Owl2, mPLUG-Owl3.
Specifically, the 7B/13B models were LLaVA-v1.5, while the 34B model adopted LLaVA-v1.6 (since v1.5 lacks a 34B variant), all executed on RTX 3090s.
For accelerated inference, 4-bit quantization is recommended to significantly reduce processing time while maintaining acceptable accuracy.

### Software Dependencies
Our experiments utilized Python 3.10, PyTorch 2.0.1, and the Ubuntu 20.04.4 LTS operating system.

## Installation Guide

### mPLUG-Owl2
1.Clone this repository and navigate to mPLUG-Owl2 folder
```
git clone https://github.com/X-PLUG/mPLUG-Owl.git
cd mPLUG-Owl/mPLUG-Owl2
```
2.Install Package
```
conda create -n mplug_owl2 python=3.10 -y
conda activate mplug_owl2
pip install --upgrade pip
pip install -e .
```
3.Install additional packages for training cases
```
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```


### mPLUG-Owl3
1.Clone this repository and navigate to mPLUG-Owl3 folder
```
git clone https://github.com/X-PLUG/mPLUG-Owl.git
cd mPLUG-Owl/mPLUG-Owl3
```
2.Install Package
```
conda create -n mplug_ow3 python=3.10 -y
conda activate mplug_owl3
pip install -r requirements.txt
```
3.Execute the demo
```
python gradio_demo.py
```


### LLaVA
1.Clone this repository and navigate to LLaVA folder
```
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA
```
2.Install Package
```
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
```
3.Install additional packages for training cases
```
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

 When using Evidential Conflict to quantify the uncertainty of large language models, model weight files are required. For ease of computation, we have placed these weight files on Hugging Face at: https://huggingface.co/datasets/thuang5288/PRE-HAL/tree/main/model_weights.

## Transformers source code modifying instruction
Since our experiments involve extracting information from the Transformers decoder, reproducing our work requires modifying the Transformers source code to simultaneously return the model's output probability distribution and the state of the decoder's final hidden layer during inference.

Taking the mPLUG-Owl2 as an example, first we need to determine the location of the Transformers source code. On my machine, its location is /data/username/miniconda3/envs/mplug_owl2/lib/python3.10/site-packages/transformers/generation/utils.py.
Then we need to determine the sampling method used by the model and enter this function.
For example, when using mPLUG-Owl2, we need to enter the ```sample``` function, then extract information such as the probability of the next token, the probability distribution, and the last hidden states of the decoder for return.
Which specific function to enter should be determined based on the model's official documentation or by using code debugging tools to step into the function gradually.


## Demo
At present, we have not yet developed a Demo.

## Further Instructions
Due to the inconsistent output formats of LVLMs (where responses may not include option letters like A, B, C, D), we strongly recommend using the GPT-4o API for auxiliary judgment. The standard experimental workflow is as follows: first, use the LVLMs for inference and store the required experimental data; second, call the GPT-4o API to match the model's responses with provided options, determining correctness (0 for no hallucination, 1 for hallucination); third, use measure.py to calculate uncertainty metrics and save the results; finally, compute the AUROC (Area Under the Receiver Operating Characteristic Curve) score based on the correctness labels and uncertainty metrics.

### Repository Structure
This respository is devided into five files, which are "infer", "infer_results", "measures", "models", "model_weights".
Among them, the "infer" folder stores the code for model inference. 
The "infer_results" folder stores the results generated by running the model inference files.
The "model_weights" folder stores the weights of the last hidden layer of each model.
The "models" folder stores the files required for the deployment of each model.
The "measures" folder contains code for processing model inference results, including code for verifying the correctness of model outputs using GPT-4o, code for calculating various uncertainty metrics, and code for computing evaluation metrics such as AUROC and ACC on the final results.
